# Тестовое Задание в Tinkoff Lab

Это тестовое задание в Tinkoff Lab на трек Domain-Specific Content Generation.

## Введение

DreamBooth - способ файнтюна диффузионных моделей на конеретный объект. Он позволяет, на основе небольшого датасета из 5-10 примеров, добавить в вокабуляр модели новый токен, соответствующий этому объекту, и использовать его для условной генерации этого объекта в новых обстоятельствах, с другим фоном, другими свойствами, и т.д.
В целом, кастомизация моделей под конкретные объекты - это очень важное и популярное направление развития text-to-image моделей.
--Это нужно для того и того.--
Кроме того, это очень популярно - существуют целые сообщества и репозитории с весами Stable Diffusion, дообученных на определенные объекты (например, [1](https://huggingface.co/sd-dreambooth-library) или [2](civit.ai)).

[DreamBooth](https://arxiv.org/abs/2208.12242), статья, которая рассматривается в этом задании, - не первая, пытающаяся применить идею кастомизации под объект. Авторы обширно ссылаются на [Textual Inversion](https://arxiv.org/abs/2208.01618), более ранний и более простой метод. По сути, это просто P-Tune для диффузионных моделей. P-Tune это метод дообучения из NLP. Суть в том, чтобы заморозить все веса модели, а обучать только непрерывные токены промпта для каждого задания, вместо того чтобы подбирать подводку вручную. В контексте диффузионных моделей, это означает найти такой токен (или несколько), который бы "означал" конкретный объект, и использовать его для инференса других картинок.

DreamBooth, в отличии от Textual Inversion, учит не только эмбеддинги, но и все веса модели целиком. Авторы утверждают, что это лучше, так как не всю информацию можно уместить в пространство токенов языковой модели. Из-за полного файнтюна, возникает проблема concept drift, когда модель забывает, как выглядят обычные объекты, и из-за этого теряет в экспрессивности, короче говоря, переобучается. Чтобы это преодолеть, применяется prior preservaion loss, когда параллельно с обычным лоссом на новом объекте модель учится генерировать обычные объекты.

Результаты получаются красивые, лучше, чем у Textual Inversion. У модели получается изменять как контекст изображения, так и свойства самого объекта, сохраняя при этом его отличительные черты. При этом, "безусловные" изображения нового объекты вообще не отличимы от оригинала.

Авторы применяют полноценный файнтюн. Это довольно вычислительно затратный метод, особенно по сравнению с легким TI. Однако, есть LoRA.Это техника дообучения, тоже изначально придуманная для NLP, но тоже очень популярная в SD коммьюнити.
Идея в том, чтобы обучать не матрицу весов $W$, а $\varDelta W$. Это эквивалентно полному дообучению, но оказалось, что на практике эта дельта часто бывает очень низкого ранга. Разложение $\varDelta W$ на произведение матриц ранга в десятки раз ниже полного, оказывается очень эффективно в плане вычислений, что позволяет обучать гиганские модели на потребительских видеокартах.

Ничего не мешает применить LoRA и к Stable Diffusion.

## Эксперименты

Для обучения и инференса моделей я использовал Google Colab, ноутбук находится в ./train_notebook.ipynb. Так же я пытался запустить его локально на своей 1080 Ti (environment колаба можно имитировать контейнером), но обучение занимало полтора часа, поэтому остановалися на колабе. Еще запускал AUTOMATIC1111, но плагин DreamBooth оказался недостаточно гибким, там, например, не было возможности менять коэффициенты LoRA, которые оказались важными параметрами и итоге.

Картинки тренировочных датасетов можно найти в папке /train_images.

### Пример из статьи

Во-первых, я хотел повторить результаты статьи, полученные полным файнтюном, с помощью LoRA; поэтому первый эксперимент - датасет с собачкой из примера в статье.

Из-за другого метода мне не подходят гиперпараметры, которые используют авторы.
Есть блоги, вводно описывающие применение LoRA к DreamBooth ([1](https://github.com/bmaltais/kohya_ss/blob/master/train_db_README.md), [2](https://huggingface.co/blog/lora)). Они все говорят, что гиперпараметры очень чувствительны, и я в своих экспериментых тоже в этом удостоверился.

Есть 2 набора гиперпараметров: обучения и инференса. Авторы блогов говорят, что главное - не переобучаться, то есть количество шагов обучения и learning rate очень важны. Кроме того, интересно заметить, часто для задачи обучения одному объекту LoRA используют без prior preservaion loss вообще, то есть просто как файнтюн на маленьком датасете. У меня такие запуски давали иногда лучше unconditional картинки (без артефактов и т.д.), но не могли собственно делать изменения, поэтому я описываю только результаты по методу из статьи, с prior preservation loss.

Гиперпараметры оказались очень нежными, и результат в точности реплицировать у меня не получилось. Картинки ниже получены с 800 шагами и lr, равным 1e-4 для UNet и 5e-5 для Text Encoder. Это почти стандартные переметры, рекомендуемые в репозитории, и действительно, они оказались оптимальными (скорее, отклонения не улучшали результат). Про параметры инференса расскажу позже.

Вот что получилось:
![image](/plots/dog_unconditional.png)

А это вариации промпта:
![image](/plots/dog_conditional.png)
```
"a photo of sks dog sleeping"
"a photo of sks dog swimming"
"a photo of sks dog in a bucket"
"a photo of sks dog getting a haircut"
"a photo of sks puppy"
"a photo of sks white dog"
"a photo of black sks dog"
"a photo of sks dog sculpture"
```

Результат выглядит неплохо, почти как в оригинальной статье. Собачка похожа, но не совсем, и изображения не всегда идельного качества. Я запускал также множество не задокументированных экспериментов, и у меня никак не получилось улучшить качество, не потеряв в похожести и экспрессивности изменений.

### Мой пример

Теперь перейдем к моему примеру. Это - мой кот Дарси.

![image](/plots/train.png)

Трейн был с теми же параметрами (800 шагов, 1e-4 и 5e-5 lr), снова другие не показали лучший результат.

![image](/plots/unconditional.png)

Опять котик в целом похож, хотя и не везде, например, шея у Дарса не белая (а в целом это, оказывается, максимально распространенный окрас [tabby cat](https://en.wikipedia.org/wiki/Tabby_cat)). Но на некоторых картинках бесформенные лапы, лишние хвосты, недорисованные глаза и тд.

![image](/plots/conditional.png)

```
"a photo of sks cat sleeping"
"a photo of sks cat underwater"
"a photo of sks cat in a bucket"
"a photo of sks cat getting a haircut"
"a photo of sks kitten"
"a photo of sks red cat"
"a photo of sphynx sks cat"
"a photo of sks cat sculpture"
```

Модель может в изменения окружения и свойств (окраса, возраста, цвета), но далеко не всегда.
Большая часть картинок получается плохо, как будто из-за недостаточного качества самой базовой модели (она плохо рисует underwater даже на базовых весах).

![image](/plots/prior.png)

На этой картинке изображен просто кот (prior) обученной модели. Судя по всему, prior preservation loss по какой-то причине не работает, как нужно. У меня не получилось понять, почему, и как сделать генерацию лучше - на абсолютно разных learning rate и датасетах получались похожие одинаковые картинки.

Дело именно в параметрах обучения (или самом датасете), потому что параметры инференсеа легко тюнить. Number of steps >30 почти никак не влиял на качество, guidance и коэффициенты влияли сильно, однако их подбор все равно не спасал плохое базовое качество.

![image](/plots/guidance.png)

Эта картинка показывает параметр силы classifier(-free) guidance от 1 до 14 по порядку. Как видно, значения 5-7 лучше всего, а дальше уже начинается контрастность. Нигде, кстати, котик так и не в ведерке, а просто рядом.

![image](/plots/coefficients.png)

Эта картинка - коэффициенты LoRA, с которыми дельта прибавляется к весам
$W+\alpha \varDelta W$.

Довольно низкий коэффициент UNet интересен, как и 0.8 коэффицент Text Encoder, но в целом это соответствует гайду из репозитория. Стоит заметить, что уменьшение lr для юнета не приводило к лучшим результатам, он очевидно недообучался и не рисовал нужного кота вообще.

Также стоит заметить, что качество датасета играет важную роль, варианты датасета с похожими фото и фото плохого качеста заметно ухудшали визуальный результат. Скорее всего, и это неоптимальный датасет, так как он отличается по композиции фото от сгенерированного из модели prior.

## Заключение

У меня не получилось повторить результаты статьи. Гиперпараметры оказались очень чувствительными и требуют значительного тюнинга. Даже если я недостаточно постарался и какой-то grid search может добиться хорошего качества, это только подтверждает что реплицировать все это непросто и нетривиально.

С другой стороны, замечательные результаты сообщества получены, как я понял, скорее несколько другими методами: просто LoRA на более большие датесеты (~30-50 картинок), либо DreamBooth как в статье, с полным файнтюном.

Также вероятно, что у авторов cherrypick датасета или random seed'ов (код не опубликован).

## Список литературы

- [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)
- [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/abs/2208.01618)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
- https://github.com/cloneofsimo/lora
- https://huggingface.co/docs/peft/task_guides/dreambooth_lora
- https://civitai.com
- https://huggingface.co/sd-dreambooth-library
